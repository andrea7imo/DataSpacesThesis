{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data spaces thesis\n",
    "## Andrea Settimo 262710\n",
    "## Leonardo Tolomei 267638\n",
    "___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "- 1 [The Forest Fires Data Set](#1)<br>\n",
    "- 2 [Data Analysis](#2)<br>\n",
    "    - 2.1 [Content of the dataset](#2.1)<br>\n",
    "    - 2.2 [The meaning of features](#2.2)<br>\n",
    "    - 2.3 [Basic statistics](#2.3)<br>\n",
    "    - 2.4 [Missing values](#2.4)<br>\n",
    "    - 2.5 [Features distributions](#2.5)<br>\n",
    "        - 2.5.1 [Categorical features](#2.5.1)<br>\n",
    "        - 2.5.2 [Numerical features](#2.5.2)<br>\n",
    "    - 2.6 [Data encoding](#2.6)<br>\n",
    "    - 2.7 [Correlation Matrix](#2.7)<br>\n",
    "    - 2.8 [Target analysis](#2.8)<br>\n",
    "- 3 [Pre-Processing](#3)<br>\n",
    "    - 3.1 [Division of prediction features](#3.1)<br>\n",
    "    - 3.2 [Dataset partirion](#3.2)<br>\n",
    "    - 3.3 [Normalization](#3.3)<br>\n",
    "    - 3.4 [Features reduction](#3.4)<br>\n",
    "    - 3.5 [PCA](#3.5)<br>\n",
    "- 4 [Model Generation](#4)<br>\n",
    "    - 4.1 [Validation procedure](#4.1)<br>\n",
    "    - 4.2 [Trainig procedure](#4.2)<br>\n",
    "    - 4.3 [Metrics](#4.3)<br>\n",
    "    - 4.4 [Models](#4.4)<br>\n",
    "        - 4.4.1 [Linear Regression](#4.4.1)<br>\n",
    "        - 4.4.2 [Regression Tree](#4.4.2)<br>\n",
    "        - 4.4.3 [Random Forest](#4.4.3)<br>\n",
    "        - 4.4.4 [SVR](#4.4.4)<br>\n",
    "        - 4.4.5 [K-NN](#4.4.5)<br>\n",
    "    - 4.5 [Results](#4.5)<br>\n",
    "        - 4.5.1 [PCA](#4.5.1)<br>\n",
    "        - 4.5.2 [Non-PCA](#4.5.2)<br>\n",
    "- 5 [Features selection](#5)<br>\n",
    "    - 5.1 [STFWI](#5.1)<br>\n",
    "    - 5.2 [STM](#5.2)<br>\n",
    "    - 5.3 [FWI](#5.3)<br>\n",
    "    - 5.4 [Weather Conditions](#5.4)<br>\n",
    "- 6 [Conclusions](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Forest Fires Data Set <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this thesis, the analysis is based on Forest Fires Data Set, which is a dataset of wildfires occurrences of Montesinho natural park, from the Tràs-os-Montes northeast region of Portugal. It is available at:\n",
    "http://archive.ics.uci.edu/ml/datasets/Forest+Fires.\n",
    "\n",
    "The research group that created this dataset, was looking for an effective and cheap way to automatically predict the burnt area of a wildfire, starting from data coming from different sources: historical spacial and temporal data, Fire Weather Index (FWI) and its components, and metereological local sensors.\n",
    "\n",
    "Here are a few samples taken from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Dataset/forestfires.csv\")\n",
    "df.head()                                      # Shows the first five rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Content of the dataset <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset size is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(\"Number of samples:\", df.shape[0])\n",
    "print(\"Number of features:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "featureNames = list(df)\n",
    "print(featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The meaning of features <a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. X: x-axis spatial coordinate within the Montesinho park map: 1 to 9;\n",
    "2. Y: y-axis spatial coordinate within the Montesinho park map: 2 to 9;\n",
    "3. month: month of the year: 'jan' to 'dec'; \n",
    "4. day: day of the week: 'mon' to 'sun';\n",
    "5. FFMC: Fine Fuel Moisture Code index (FFMC) from the FWI system: 18.7 to 96.20; \n",
    "6. DMC: Duff Moisture Code (DMC) index from the FWI system: 1.1 to 291.3; \n",
    "7. DC: Drought Code (DC) index from the FWI system: 7.9 to 860.6;\n",
    "8. ISI: Initial Spread Index (ISI) index from the FWI system: 0.0 to 56.10; \n",
    "9. temp: temperature in Celsius degrees: 2.2 to 33.30;\n",
    "10. RH: relative humidity in %: 15.0 to 100; \n",
    "11. wind: wind speed in km/h: 0.40 to 9.40;\n",
    "12. rain: outside rain in mm/m2 : 0.0 to 6.4;\n",
    "13. area: the burned area of the forest (in ha): 0.00 to 1090.84 (this feature is the target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Basic statistics <a class=\"anchor\" id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate some statistics from the dataset. A basic statistic is generated using the \"describe\" method which provides some summuries:\n",
    "- count: the number of elements present in each feature;\n",
    "- mean: the average of a feature;\n",
    "- std: the standard deviation of observation;\n",
    "- min: the minimum value of the feature;\n",
    "- 25%: the 25 percentile;\n",
    "- 50%: the 50 percentile;\n",
    "- 75%: the 75 percentile;\n",
    "- max: the maximum value of the feature.\n",
    "\n",
    "Below is reported the summary table with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Missing values <a class=\"anchor\" id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the correct implementation of algorithms is important to know if there are missing values in the dataset. If they were present, a correction to avoid the problem would be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"\\nIn this dataset there are no missing values.\\n\")\n",
    "else:\n",
    "    print(f\"\\nIn this dataset there are some missing values:\\n{missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Features distributions <a class=\"anchor\" id=\"2.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the distributions of features within the dataset.\n",
    "\n",
    "Categorical and numerical features are treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=\"area\")\n",
    "cat_columns = features.select_dtypes(include='object').columns.tolist()\n",
    "num_columns = features.select_dtypes(exclude='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Categorical features <a class=\"anchor\" id=\"2.5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Month and day are the only categorical features.\n",
    "\n",
    "For both of them, an histogram representation of value frequencies is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "for i,col in enumerate(cat_columns,1):\n",
    "    plt.subplot(1,2,i)\n",
    "    df[col].value_counts().plot.bar()\n",
    "    plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Numerical features  <a class=\"anchor\" id=\"2.5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining features are all numerical. So, for each of them, a density graph and a box plot were plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot(kind='density', subplots=True, layout=(4,3), figsize=(20,15), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots are a very compact and effective representation of data distributions. They can visualize a summary of important statistical numbers and also highlight possible outliers. A more detailed description of the representation method is shown in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/18000/1*2c21SkzJMf3frPXPAR_gZA.png\" alt=\"boxplot\" style=\"width:500px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot(kind='box', subplots=True, layout=(4,3), figsize=(15,15), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Data encoding <a class=\"anchor\" id=\"2.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete data analysis and to apply data mining algorithms, some pre-processing is required. In particular, for all algorithms except for Decision Tree and Random Forest, categorical features are transformed through a 1-of-C encoding (a.k.a. \"dummy\" encoding). This type of transformation converts a categorical feature with N possible values into N binary features, with one of them equal to 1 and all others equal to 0 for each sample.\n",
    "\n",
    "Moreover, original categorical features are converted to ordinal integers, to be able to use library functions for Decision Tree and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['day'] = pd.Categorical(df['day'])\n",
    "df['month'] = pd.Categorical(df['month'])\n",
    "\n",
    "day_dummies = pd.get_dummies(df['day'], prefix=\"day\")\n",
    "month_dummies = pd.get_dummies(df['month'], prefix=\"month\")\n",
    "\n",
    "df_dummy = df.drop(['day', 'month'], axis=1)\n",
    "df_dummy = pd.concat([df_dummy, day_dummies, month_dummies], axis=1)\n",
    "featureNames_dummy = list(df_dummy)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['day'] = le.fit_transform(df['day'])\n",
    "df['month'] = le.fit_transform(df['month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_dummy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Correlation Matrix <a class=\"anchor\" id=\"2.7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation allows us to see the relationship between two statistical variables X and Y. It takes into account the definition of covariance, which also calculates the relationship between two statistical variables, but the correlation represents the relationship in a range of values between -1 and 1. This is possible because it divides the covariance value by the standard deviation value.\n",
    "\n",
    "\n",
    "Definition of covariance between two statistical variables X and Y:\n",
    "\n",
    "\\begin{equation*}\n",
    "cov(X, Y) = E[(X - \\mu_x)(Y - \\mu_y)] = E[XY] - E[X]E[Y] = E[XY] - \\mu_x\\mu_y\n",
    "\\end{equation*}\n",
    "\n",
    "Instead, in case of two equal statistical variables:\n",
    "\n",
    "\\begin{equation*}\n",
    "cov(X, X) = Var(X) = \\sigma^2(X) = \\sigma^2_X \n",
    "\\end{equation*}\n",
    "\n",
    "So we can define Pearson's correlation between two statistical variables X and Y as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\rho_{XY} = \\frac{cov(X, Y)}{\\sigma_X\\sigma_Y}\n",
    "\\end{equation*}\n",
    "\n",
    "Instead, in case of two equal statistical variables, we can get:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\rho_{XX} = \\frac{cov(X, X)}{\\sigma_X\\sigma_X} = \\frac{\\sigma_X^2}{\\sigma_X\\sigma_X} = \\frac{\\sigma_X^2}{\\sigma_X^2} = 1\n",
    "\\end{equation*}\n",
    "\n",
    "We can also give meaning to these values:\n",
    "- if $\\rho_{XY} > 0$, the variables $X$ and $Y$ are said to be directly related; \n",
    "- if $\\rho_{XY} = 0$, the variables $X$ and $Y$ are said to be uncorrelated; \n",
    "- if $\\rho_{XY} < 0$, the variables $X$ and $Y$ are said to be inversely related.\n",
    "\n",
    "So our correlation matrix is defined simply as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\rho_{ij} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i}\\sigma_{X_j}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where X is our dataset, i and j are related to the features being evaluated.\n",
    "We can see that our final matrix will be symmetric ($\\rho_{ij} = \\rho_{ji}$) and the coefficients on the diagonal will be equal to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "correlations = df_dummy.corr()\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "im = ax.imshow(correlations)\n",
    "ax.set_xticks(np.arange(df_dummy.shape[1]))\n",
    "ax.set_yticks(np.arange(df_dummy.shape[1]))\n",
    "# label them with the respective list entries\n",
    "ax.set_xticklabels(list(df_dummy.columns))\n",
    "ax.set_yticklabels(list(df_dummy.columns))\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(df_dummy.shape[1]):\n",
    "    for j in range(df_dummy.shape[1]):\n",
    "        text = ax.text(j, i, \"{0:.2f}\".format(correlations.iloc[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel(\"Correletion value\", rotation=-90, va=\"bottom\")\n",
    "\n",
    "ax.set_title(\"Heatmap for correlation matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Target analysis <a class=\"anchor\" id=\"2.8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target of this dataset is the burnt area in ha.\n",
    "\n",
    "Note that a zero value means an area lower than: \\begin{equation*}\\frac{1ha}{100} = 100m^2\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "df['area'].hist(bins=50)\n",
    "plt.title('Histogram of burnt area (in ha)')\n",
    "plt.subplot(1,2,2)\n",
    "df['area'].plot(kind=\"box\")\n",
    "plt.title('Boxplot of burnt area (in ha)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burnt area is very skewed, with the majority of the fires presenting a small size. So, the logarithm function can be applied to reduce skewness and improve symmetry.\n",
    "\n",
    "More precisely, the transformation\n",
    "\n",
    "\\begin{equation*}\n",
    "t(x) = \\ln{(x+1)}\n",
    "\\end{equation*}\n",
    "\n",
    "can be applied and model predictions will need to be post-processed with the inverse transformation:\n",
    "\n",
    "\\begin{equation*}\n",
    "t^{-1}(x) = e^x-1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy['area'] = df['area'] = np.log1p(df['area'])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "df['area'].hist(bins=50)\n",
    "plt.title('Histogram of log(1 + area) transform of the area')\n",
    "plt.subplot(1,2,2)\n",
    "df['area'].plot(kind=\"box\")\n",
    "plt.title('Boxplot of log(1 + area) transform of the area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Division of prediction features <a class=\"anchor\" id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to divide the column representing the target from the rest of the columns.\n",
    "The target value is used to perform the learning of the model and also the test on it. In our dataset, the target value is represented by the area feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# X not encoded for Decision Tree and Random Forest\n",
    "X_DT = df.drop(columns=['area']).values\n",
    "# X encoded for remaining algorithms\n",
    "X = df_dummy.drop(columns=['area']).values\n",
    "\n",
    "Y = df['area'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dataset partition <a class=\"anchor\" id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the study of the various models we must understand how the newly generated models can perform on data they have never seen.\n",
    "To do this, we can divide the dataset into two parts:\n",
    "- Training set: is the set where our model goes to learn;\n",
    "- Test set: is the part of dataset where we can evaluate our models. \n",
    "\n",
    "To do this we have to define which parameter is best for splitting the training set and the test set. One possible solution is to take a set of values (which is the split proportion) and try them with the algorithms that we are going to use on our training set. We will choose the proportion that will give a better result on the test.\n",
    "The various attempts made, showed that the best proportion found is 50% of data for the training set and 50% of data for the test set.\n",
    "\n",
    "The division is implemented using the train_test_split library function which, given a proportion for test set size compared to that of training, returns the divided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/size training e test.png\" alt=\"boxplot\" style=\"width:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.50, random_state=0)\n",
    "X_train_DT, X_test_DT, Y_train_DT, Y_test_DT = train_test_split( X_DT, Y, test_size=0.50, random_state=0)\n",
    "\n",
    "print(\"The size of the training set is:\", len(X_train))\n",
    "print(\"The size of the test set is:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalization <a class=\"anchor\" id=\"3.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a type of data transformation, whose values are adapted into a smaller range of numeric values, typically a range between [-1, 1] or [0, 1] is used. It is used in attributes to have the same reference scale because it is useful in the concept of distance.\n",
    "In this case the z-score transformation can be chosen and it is defined as:\n",
    "\\begin{equation*}\n",
    "z = \\frac{X - μ}{σ}\n",
    "\\end{equation*}\n",
    "\n",
    "Using the SandardScaler library class, we can calculate the transformation using the fit method on the training set (result of the division of the previous step) and then apply the changes on the training set and test set.\n",
    "\n",
    "Also this transformation was applied to all algorithms except for Decision Tree and Random Forest, which do not explicitly need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-compute post-processed Ys\n",
    "Y_train_post = np.expm1(Y_train)\n",
    "Y_test_post = np.expm1(Y_test)\n",
    "Y_train_post_DT = np.expm1(Y_train_DT)\n",
    "Y_test_post_DT = np.expm1(Y_test_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrivere qualcosa... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=\"area\")\n",
    "features[['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']] = scaler.fit_transform(features[['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']])\n",
    "features.plot(kind='box', subplots=True, layout=(4,3), figsize=(15,15), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Features reduction <a class=\"anchor\" id=\"3.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have many features we can run into a performance and memory space problem. It must be considered that not all features are relevant to the problem to be solved, to select which features to use two techniques can be used:\n",
    "\n",
    "- Dimensionality reduction: it is based on mathematical techniques that allow the recombination of the original features (e.g: PCA, Fisher LDA).\n",
    "- Feature selection: it is based on taking a subset of features. From a mathematical point of view it means designing the points in a new space called feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 PCA <a class=\"anchor\" id=\"3.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis is a decomposition technique that extracts from a multivariate dataset a set of successive orthogonal components that explain a maximum amount of variance.\n",
    "\n",
    "In other words, it is an approximation of the dataset in a lower dimensional space, preserving largest variances in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPCACumulativeExplainedVariance(pca):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.bar(np.arange(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, color='orange')\n",
    "    plt.plot(np.arange(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.title('Explained variance by different principal components')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Explained variance in percent')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPCABiplot(score, y, coeff, labels=None):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def makePCA(X_train_, Y_train_, X_test_, labels=None):\n",
    "    pca = PCA(n_components=.95)\n",
    "    pca.fit(X_train_)\n",
    "    X_train_pca_ = pca.transform(X_train_)\n",
    "    X_test_pca_ = pca.transform(X_test_)\n",
    "    \n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(X_train_)\n",
    "    X_train_pca_full_ = pca_full.transform(X_train_)\n",
    "    \n",
    "    plotPCACumulativeExplainedVariance(pca_full)\n",
    "    plotPCABiplot(X_train_pca_full_[:,0:2], Y_train_, np.transpose(pca_full.components_[0:2,:]), labels)\n",
    "    \n",
    "    return X_train_pca_, X_test_pca_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, two training dataset version are present: one is similar to the original dataset, and is dedicated to Decision Tree and Random Forest; the other is encoded and normalized, and is dedicated to all other algorithms.\n",
    "\n",
    "Each time PCA is applyed to one of these two dataset versions, two graphs are plotted. One contains a bar plot for individual explained variance and a line plot for cumulative explained variance. The individual explained variance is the variance explained by a single component in percentage, and the cumulative explained variance is the cumulative sum of these percentages. The other graph contains a PCA biplot, which is a merge of a PCA score plot and a loading plot. [TODO]\n",
    "\n",
    "Components are ordered in terms of variance, showing how the algorithm proceeded during the selection. The plot stops at 95% of cumulative variance, which is the target variance that we selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_pca, X_test_pca = makePCA(X_train, Y_train, X_test, featureNames_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on non-encoded dataset (for Decision Tree and Random Forest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_DT, X_test_pca_DT = makePCA(X_train_DT, Y_train_DT, X_test_DT, featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Model Generation <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Validation procedure <a class=\"anchor\" id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the hyperparameters of our algorithms we must use a validation procedure. There are two types of validation:\n",
    "- Hold out: the training set is divided into two parts, one for training and another one for model validation to understand if those hyperparameters are better than others. usually for division between training and validation set the following proportion is used: ⅔ for the training and ⅓ for the validation set. This methodology is fine when you have a huge amount of data and you get a validation of the models in a fast but not very accurate way;\n",
    "- K-fold: this technique allows us to validate our models more accurately. This technique allows you to generate K sets of training and validation sets that are different from each other, where each of them always uses different data within the validation set, so that the assessment of the prediction can be generalized as much as possible. In doing so, our evaluation generalizes the goodness of our model of generalizing data that it has never seen. This is a very good thing, so you can get good results when testing. Then k models are created and each of them is tested with a validitation set (relative to it) having size 1 / k of the complete trainig set. Instead, the training set used in this phase has a size equal to (k-1) / k. In this case we have decided to use this type of validation as the size of the dataset is not large. A figure is given to best summarize this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Validation.png\" alt=\"boxplot\" style=\"width:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analyses were carried out with a k equal to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training procedure <a class=\"anchor\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the validation procedure is done, the validation and the test set are combined to be able to train with the best hyperparameters found. Once the training is over, you go to see how good your model is on data that it has never seen, so you go to test it with the test set (data that we have put aside already before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Training and Test.png\" alt=\"boxplot\" style=\"width:75%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Metrics <a class=\"anchor\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics used to evaluate our models are:\n",
    "- Mean Absolute Error (MAE):\n",
    "\n",
    "\\begin{equation*}\n",
    "MAE = \\frac{1}{n} \\sum^n_{j=1}|y_j - \\hat{y_j}|\n",
    "\\end{equation*}\n",
    "\n",
    "- Root mean squared error (RMSE):\n",
    "\n",
    "\\begin{equation*}\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{j=1}(y_j - \\hat{y_j})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Models  <a class=\"anchor\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Linear Regression  <a class=\"anchor\" id=\"4.4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of algorithm is the simplest that can be found in the habit of regression, it is based on a linear relationship between X and Y, but this in reality is unlikely to be possible. \n",
    "We can define our model starting from the one-dimensional case:\n",
    "\n",
    "\\begin{equation*}\n",
    "Y = \\beta_0 + \\beta_1X + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "we must try to estimate the coefficients of the model ($\\hat{\\beta_0}$ and $\\hat{\\beta_1}$), so as to obtain an estimate of the target ($\\hat{y}$).\n",
    "The algorithm is based on the calculation of the residuals:\n",
    "\n",
    "\\begin{equation*}\n",
    "e_i = y_i - \\hat{y_i}, \\ i \\in n \\ (number \\ of \\ samples)\n",
    "\\end{equation*}\n",
    "\n",
    "We can define our loss functions that we have to optimize:\n",
    "\n",
    "\\begin{equation*}\n",
    "Loss(\\hat{y}, y) = \\sum_{i}^n (y_i - \\hat{\\beta_0} + \\hat{\\beta_1}x_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "This mode is called least square criterion which allows you to choose the coefficients $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ that minimize the loss function.\n",
    "In the multidimensional case, we can estimate p coefficients: $\\hat{\\beta_0},\\ \\hat{\\beta_1},\\ ...,\\ \\hat{\\beta_p}$.\n",
    "the resulting model is given by the following formulation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\ ...\\ +\\hat{\\beta_p}x_p\n",
    "\\end{equation*}\n",
    "\n",
    "The loss function does not change, you must always minimize taking into account the new formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def makeLinearRegression(X_train_, Y_train_, Y_train_post_, X_test_, Y_test_post_):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_, Y_train_)\n",
    "    \n",
    "    Y_predict = model.predict(X_test_)\n",
    "    # Post-processing\n",
    "    Y_predict_post = np.expm1(Y_predict)   \n",
    "    print(\"Test results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_test_post_, Y_predict_post))\n",
    "    \n",
    "    Y_predict_train = model.predict(X_train_)\n",
    "    # Post-processing\n",
    "    Y_predict_train_post = np.expm1(Y_predict_train)    \n",
    "    print(\"Train results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_train_post_, Y_predict_train_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_train_post_, Y_predict_train_post))\n",
    "    \n",
    "    return np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)), metrics.mean_absolute_error(Y_test_post_, Y_predict_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca_RMSE, lr_pca_MAE = makeLinearRegression(X_train_pca, Y_train, Y_train_post, X_test_pca, Y_test_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lr_RMSE, lr_MAE = makeLinearRegression(X_train, Y_train, Y_train_post, X_test, Y_test_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Regression Tree <a class=\"anchor\" id=\"4.4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is a non-parametric method used for classification and regression.\n",
    "\n",
    "The regression version differs from the classification one for the output y, which is a floating point instead of an integer number. The result is a branching structure that represents a set of rules, distinguishing values in a hierarchical form.\n",
    "\n",
    "Regression Trees are easy to implement and interpret, as they always produce a readable output (as shown for each run). They are only sensible to missing values, so they do not require much data preparation and they are natively able to handle categorical data. However, they easily overfit and their simplicity does not make them suitable for all kinds of problems.\n",
    "\n",
    "For the algorithm construction, a stopping rule and a split evaluation criterion can be defined.\n",
    "We adopted maximum depth as stopping rule, trying different values (i.e.: 1, 2, 3, 5, 10, 15, 20) and we tested three split evaluation criteria: MSE, Friedman MSE and MAE.\n",
    "\n",
    "MAE was already defined in section 4.3. Also MSE is like the already defined RMSE, without the root. Friedman MSE, instead, uses MSE with Friedman's improvement score for potential splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def makeDecisionTree(X_train_, Y_train_, Y_train_post_, X_test_, Y_test_post_):\n",
    "    decisionTree = GridSearchCV(estimator=DecisionTreeRegressor(),\n",
    "                                param_grid={'max_depth': [1, 2, 3, 5, 10, 15, 20], 'criterion': [\"mse\", \"friedman_mse\", \"mae\"]},\n",
    "                                cv=10, iid=False)\n",
    "    decisionTree.fit(X_train_, Y_train_)\n",
    "    \n",
    "    Y_predict = decisionTree.predict(X_test_)\n",
    "    # Post-processing\n",
    "    Y_predict_post = np.expm1(Y_predict)   \n",
    "    print(\"Test results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_test_post_, Y_predict_post))\n",
    "    \n",
    "    Y_predict_train = decisionTree.predict(X_train_)\n",
    "    # Post-processing\n",
    "    Y_predict_train_post = np.expm1(Y_predict_train)    \n",
    "    print(\"Train results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_train_post_, Y_predict_train_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_train_post_, Y_predict_train_post))\n",
    "    \n",
    "    tree.plot_tree(decisionTree.best_estimator_)\n",
    "    \n",
    "    return np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)), metrics.mean_absolute_error(Y_test_post_, Y_predict_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pca_RMSE, dt_pca_MAE = makeDecisionTree(X_train_pca_DT, Y_train_DT, Y_train_post_DT, X_test_pca_DT, Y_test_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dt_RMSE, dt_MAE = makeDecisionTree(X_train_DT, Y_train_DT, Y_train_post_DT, X_test_DT, Y_test_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Random forest <a class=\"anchor\" id=\"4.4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensamble version of the Decision Tree algorithm. It produces T unpruned Decision Trees, using random features selection from bootstrap training samples. The predictor averages the output of all the trees. It usually produces better results then a single Decision Tree, with an higher predictive accuracy and better control of over-fitting.\n",
    "\n",
    "Also in this case, we adopted max depth as stopping criterion, and we tried different depths (i.e.: 1, 2, 3, 5, 10, 15, 20) and different split quality measures (i.e: MAE, MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRandomForest(X_train_, Y_train_, Y_train_post_, X_test_, Y_test_post_):\n",
    "    randomForest = GridSearchCV(estimator=RandomForestRegressor(n_estimators=100),\n",
    "                                param_grid={'max_depth': [1, 2, 3, 5, 10, 15, 20], 'criterion': [\"mse\", \"mae\"]},\n",
    "                                cv=10, iid=False, n_jobs=-1)\n",
    "    randomForest.fit(X_train_, Y_train_)\n",
    "    \n",
    "    Y_predict = randomForest.predict(X_test_)\n",
    "    # Post-processing\n",
    "    Y_predict_post = np.expm1(Y_predict)   \n",
    "    print(\"Test results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_test_post_, Y_predict_post))\n",
    "    \n",
    "    Y_predict_train = randomForest.predict(X_train_)\n",
    "    # Post-processing\n",
    "    Y_predict_train_post = np.expm1(Y_predict_train)    \n",
    "    print(\"Train results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_train_post_, Y_predict_train_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_train_post_, Y_predict_train_post))\n",
    "    \n",
    "    return np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)), metrics.mean_absolute_error(Y_test_post_, Y_predict_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca_RMSE, rf_pca_MAE = makeRandomForest(X_train_pca_DT, Y_train_DT, Y_train_post_DT, X_test_pca_DT, Y_test_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_RMSE, rf_MAE = makeRandomForest(X_train_DT, Y_train_DT, Y_train_post_DT, X_test_DT, Y_test_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4 SVR <a class=\"anchor\" id=\"4.4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine (SVM) algorithm can also be used in the case of regression. \n",
    "In this case you take the mode of support vector regression and use the same concepts used in the classification.\n",
    "At the end of the process you always get a maximal margin, also keeping in mind a possible error tolerance.\n",
    "\n",
    "In this case the problem turns:\n",
    "\n",
    "\\begin{equation*}\n",
    "min \\ \\frac{1}{2}|w|^2 + C \\sum^N_i(\\xi_i + \\xi_i^*) \\\\\n",
    "subject \\ to:\n",
    "y_i - \\langle w,x_i \\rangle - b < \\epsilon + \\xi_i; \\ \n",
    "\\langle w,x_i \\rangle - b - y_i < \\epsilon + \\xi_i^*; \\ \n",
    "\\xi_i,\\xi_i^* \\geq 0\n",
    "\\end{equation*}\n",
    "\n",
    "$\\epsilon$: it is a margin of error, which allows you to consider the points in the margin as tolerated errors.\n",
    "\n",
    "$\\xi_i$: it is called slack variables, in the case of regression it is a value assigned to points outside the margin.\n",
    "\n",
    "$C$: it describes how many errors are allowed during the training time. \n",
    "\n",
    "The image below represents the meaning of the epsilon parameter and the slack variables.\n",
    "\n",
    "<img src=\"img/SVR.png\" alt=\"boxplot\" style=\"width:50%;\" />\n",
    "\n",
    "The mapping allows to map the points into the space where they are linearly separable and then apply the dot product. \n",
    "However, this operation can be very expensive.\n",
    "Also to solve the problem and to avoid mapping points in another space, you can use the concept of kernel.\n",
    "The kernel function must meet the following requirement:\n",
    "\n",
    "\\begin{equation*}\n",
    "k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle\n",
    "\\end{equation*}\n",
    "\n",
    "Using a kernel is more efficient than mapping into another space. \n",
    "In our case we went to consider two types of kernels:\n",
    "- Linear kernel:\n",
    "\n",
    "\\begin{equation*}\n",
    "k(x, x') = \\langle x, x' \\rangle\n",
    "\\end{equation*}\n",
    "\n",
    "- Residual Basis Function:\n",
    "\n",
    "\\begin{equation*}\n",
    "k(x, x') = e^{-\\gamma |x-x'|^2}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSVR(X_train_, Y_train_, Y_train_post_, X_test_, Y_test_post_):\n",
    "    svr = GridSearchCV(estimator=SVR(),\n",
    "                                param_grid={'C': [0.001, 0.01, 0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'epsilon': [0.01, 0.1, 1, 10, 100]},\n",
    "                                cv=10, iid=False, n_jobs=-1)\n",
    "    svr.fit(X_train_, Y_train_)\n",
    "    \n",
    "    print(\"Best parameters: \", svr.best_params_)\n",
    "    Y_predict = svr.predict(X_test_)\n",
    "    # Post-processing\n",
    "    Y_predict_post = np.expm1(Y_predict)   \n",
    "    print(\"Test results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_test_post_, Y_predict_post))\n",
    "    \n",
    "    Y_predict_train = svr.predict(X_train_)\n",
    "    # Post-processing\n",
    "    Y_predict_train_post = np.expm1(Y_predict_train)    \n",
    "    print(\"Train results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_train_post_, Y_predict_train_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_train_post_, Y_predict_train_post))\n",
    "    \n",
    "    return np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)), metrics.mean_absolute_error(Y_test_post_, Y_predict_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_pca_RMSE, svr_pca_MAE = makeSVR(X_train_pca, Y_train, Y_train_post, X_test_pca, Y_test_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_RMSE, svr_MAE = makeSVR(X_train, Y_train, Y_train_post, X_test, Y_test_post)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.5 K-NN <a class=\"anchor\" id=\"4.4.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also this algorithm can be extended to the case of regression. This technique allows to fix k points of the training set to evaluate the prediction.\n",
    "\n",
    "A possible pseudo algorithm can be:\n",
    "\n",
    "    for p in point:\n",
    "    \n",
    "        select k points close the point p\n",
    "        \n",
    "        assign:\n",
    "\\begin{equation*}\n",
    "\\hat{y}:  \\hat{y} = \\frac{1}{K}\\sum_i^n y_i\n",
    "\\end{equation*}\n",
    "   \n",
    "As can be seen from the pseudo-algorithm, this type of model is based on the definition of distance. In our case we went to evaluate two types of distances: \n",
    "- Manhattan (p = 1);\n",
    "- Euclidian (p = 2).\n",
    "\n",
    "\\begin{equation*}\n",
    "L_p(x,y) = (\\sum_i^d(x_i - y_i)^p)^{\\frac{1}{p}}\n",
    "\\end{equation*}\n",
    "\n",
    "In addition to two types of distances we went to evaluate different k values: 1, 3, 5, 7, 9, 13, 15, 17 and 19.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeKNN(X_train_, Y_train_, Y_train_post_, X_test_, Y_test_post_):\n",
    "    knn = GridSearchCV(estimator=KNeighborsRegressor(),\n",
    "                                param_grid={'n_neighbors': [1, 3, 5, 7, 9, 13, 15, 17, 19], 'p': [1, 2]},\n",
    "                                cv=10, iid=False, n_jobs=-1)\n",
    "    knn.fit(X_train_, Y_train_)\n",
    "    \n",
    "    Y_predict = knn.predict(X_test_)\n",
    "    # Post-processing\n",
    "    Y_predict_post = np.expm1(Y_predict)   \n",
    "    print(\"Test results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_test_post_, Y_predict_post))\n",
    "    \n",
    "    Y_predict_train = knn.predict(X_train_)\n",
    "    # Post-processing\n",
    "    Y_predict_train_post = np.expm1(Y_predict_train)    \n",
    "    print(\"Train results:\")\n",
    "    print(\"\\tRMSE:\", np.sqrt(metrics.mean_squared_error(Y_train_post_, Y_predict_train_post)))  # RMSE\n",
    "    print(\"\\tMAE:\", metrics.mean_absolute_error(Y_train_post_, Y_predict_train_post))\n",
    "    \n",
    "    return np.sqrt(metrics.mean_squared_error(Y_test_post_, Y_predict_post)), metrics.mean_absolute_error(Y_test_post_, Y_predict_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca_RMSE, knn_pca_MAE = makeKNN(X_train_pca, Y_train, Y_train_post, X_test_pca, Y_test_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_RMSE, knn_MAE = makeKNN(X_train, Y_train, Y_train_post, X_test, Y_test_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Results <a class=\"anchor\" id=\"4.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResult(RMSEs, RMSEs_pca, MAEs, MAEs_pca):    \n",
    "    \n",
    "    labels = ['Linear Regression', 'Regression Tree', 'Random Forest', 'SVR', 'K-NN']\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    rects1 = ax.bar(x - width/2, RMSEs, width, label='Without PCA')\n",
    "    rects2 = ax.bar(x + width/2, RMSEs_pca, width, label='PCA')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.set_title('RMSE results')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(min(RMSEs) - 0.1, max(RMSEs) + 0.1)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    labels = ['Linear Regression', 'Regression Tree', 'Random Forest', 'SVR', 'K-NN']\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    rects1 = ax.bar(x - width/2, MAEs, width, label='Without PCA', color='green')\n",
    "    rects2 = ax.bar(x + width/2, MAEs_pca, width, label='PCA', color='red')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('MAE')\n",
    "    ax.set_title('MAE results')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(min(MAEs) - 0.1, max(MAEs) + 0.1)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = [lr_RMSE, dt_RMSE, rf_RMSE, svr_RMSE, knn_RMSE]  \n",
    "RMSEs_pca = [lr_pca_RMSE, dt_pca_RMSE, rf_pca_RMSE, svr_pca_RMSE, knn_pca_RMSE] \n",
    "MAEs = [lr_MAE, dt_MAE, rf_MAE, svr_MAE, knn_MAE]\n",
    "MAEs_pca = [lr_pca_MAE, dt_pca_MAE, rf_pca_MAE, svr_pca_MAE, knn_pca_MAE]\n",
    "plotResult(RMSEs, RMSEs_pca, MAEs, MAEs_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Features selection <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several feature combinations were tested, in order to reduce the dimensionality of the dataset and test the impact of variables.\n",
    "\n",
    "Moreover, we tried to apply also PCA, to test if an even smaller dataset could perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 STFWI <a class=\"anchor\" id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFWI – using spatial, temporal and the four FWI components:\n",
    "- X\n",
    "- Y\n",
    "- month\n",
    "- day \n",
    "- FFMC\n",
    "- DMC\n",
    "- DC\n",
    "- ISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_STFWI = df_dummy.drop(columns=['temp', 'RH', 'wind', 'rain', 'area'])\n",
    "featureNames_dummy = list(X_STFWI)\n",
    "X_STFWI = X_STFWI.values\n",
    "\n",
    "X_STFWI_DT = df.drop(columns=['temp', 'RH', 'wind', 'rain', 'area'])\n",
    "featureNames = list(X_STFWI_DT)\n",
    "X_STFWI_DT = X_STFWI_DT.values\n",
    "\n",
    "Y_STFWI = df['area'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_STFWI, X_test_STFWI, Y_train_STFWI, Y_test_STFWI = train_test_split( X_STFWI, Y_STFWI, test_size=0.30, random_state=0)\n",
    "X_train_STFWI_DT, X_test_STFWI_DT, Y_train_STFWI_DT, Y_test_STFWI_DT = train_test_split( X_STFWI_DT, Y_STFWI, test_size=0.30, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train_STFWI)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train_STFWI = scaler.transform(X_train_STFWI)\n",
    "X_test_STFWI = scaler.transform(X_test_STFWI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute post-processed Ys\n",
    "Y_train_STFWI_post = np.expm1(Y_train_STFWI)\n",
    "Y_test_STFWI_post = np.expm1(Y_test_STFWI)\n",
    "Y_train_STFWI_post_DT = np.expm1(Y_train_STFWI_DT)\n",
    "Y_test_STFWI_post_DT = np.expm1(Y_test_STFWI_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_STFWI, X_test_pca_STFWI = makePCA(X_train_STFWI, Y_train_STFWI, X_test_STFWI, featureNames_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on non-encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_STFWI_DT, X_test_pca_STFWI_DT = makePCA(X_train_STFWI_DT, Y_train_STFWI_DT, X_test_STFWI_DT, featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "lr_pca_RMSE_STFWI, lr_pca_MAE_STFWI = makeLinearRegression(X_train_pca_STFWI, Y_train_STFWI, Y_train_STFWI_post, X_test_pca_STFWI, Y_test_STFWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "lr_RMSE_STFWI, lr_MAE_STFWI = makeLinearRegression(X_train_STFWI, Y_train_STFWI, Y_train_STFWI_post, X_test_STFWI, Y_test_STFWI_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "dt_pca_RMSE_STFWI, dt_pca_MAE_STFWI = makeDecisionTree(X_train_pca_STFWI_DT, Y_train_STFWI_DT, Y_train_STFWI_post_DT, X_test_pca_STFWI_DT, Y_test_STFWI_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "dt_RMSE_STFWI, dt_MAE_STFWI = makeDecisionTree(X_train_STFWI_DT, Y_train_STFWI_DT, Y_train_STFWI_post_DT, X_test_STFWI_DT, Y_test_STFWI_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "rf_pca_RMSE_STFWI, rf_pca_MAE_STFWI = makeRandomForest(X_train_pca_STFWI_DT, Y_train_STFWI_DT, Y_train_STFWI_post_DT, X_test_pca_STFWI_DT, Y_test_STFWI_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "rf_RMSE_STFWI, rf_MAE_STFWI = makeRandomForest(X_train_STFWI_DT, Y_train_STFWI_DT, Y_train_STFWI_post_DT, X_test_STFWI_DT, Y_test_STFWI_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "svr_pca_RMSE_STFWI, svr_pca_MAE_STFWI = makeSVR(X_train_pca_STFWI, Y_train_STFWI, Y_train_STFWI_post, X_test_pca_STFWI, Y_test_STFWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "svr_RMSE_STFWI, svr_MAE_STFWI = makeSVR(X_train_STFWI, Y_train_STFWI, Y_train_STFWI_post, X_test_STFWI, Y_test_STFWI_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "knn_pca_RMSE_STFWI, knn_pca_MAE_STFWI = makeKNN(X_train_pca_STFWI, Y_train_STFWI, Y_train_STFWI_post, X_test_pca_STFWI, Y_test_STFWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "knn_RMSE_STFWI, knn_MAE_STFWI = makeKNN(X_train_STFWI, Y_train_STFWI, Y_train_STFWI_post, X_test_STFWI, Y_test_STFWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = [lr_RMSE_STFWI, dt_RMSE_STFWI, rf_RMSE_STFWI, svr_RMSE_STFWI, knn_RMSE_STFWI]  \n",
    "MAEs = [lr_MAE_STFWI, dt_MAE_STFWI, rf_MAE_STFWI, svr_MAE_STFWI, knn_MAE_STFWI]\n",
    "RMSEs_pca = [lr_pca_RMSE_STFWI, dt_pca_RMSE_STFWI, rf_pca_RMSE_STFWI, svr_pca_RMSE_STFWI, knn_pca_RMSE_STFWI] \n",
    "MAEs_pca = [lr_pca_MAE_STFWI, dt_pca_MAE_STFWI, rf_pca_MAE_STFWI, svr_pca_MAE_STFWI, knn_pca_MAE_STFWI]\n",
    "plotResult(RMSEs, RMSEs_pca, MAEs, MAEs_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 STM <a class=\"anchor\" id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STM – with the spatial, temporal and the four weather variables:\n",
    "- X\n",
    "- Y\n",
    "- month\n",
    "- day \n",
    "- temp\n",
    "- RH\n",
    "- wind\n",
    "- rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_STM = df_dummy.drop(columns=['FFMC', 'DMC', 'DC', 'ISI', 'area'])\n",
    "featureNames_dummy = list(X_STM)\n",
    "X_STM = X_STM.values\n",
    "\n",
    "X_STM_DT = df.drop(columns=['FFMC', 'DMC', 'DC', 'ISI', 'area'])\n",
    "featureNames = list(X_STM_DT)\n",
    "X_STM_DT = X_STM_DT.values\n",
    "\n",
    "Y_STM = df['area'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_STM, X_test_STM, Y_train_STM, Y_test_STM = train_test_split( X_STM, Y_STM, test_size=0.30, random_state=0)\n",
    "X_train_STM_DT, X_test_STM_DT, Y_train_STM_DT, Y_test_STM_DT = train_test_split( X_STM_DT, Y_STM, test_size=0.30, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train_STM)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train_STM = scaler.transform(X_train_STM)\n",
    "X_test_STM = scaler.transform(X_test_STM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute post-processed Ys\n",
    "Y_train_STM_post = np.expm1(Y_train_STM)\n",
    "Y_test_STM_post = np.expm1(Y_test_STM)\n",
    "Y_train_STM_post_DT = np.expm1(Y_train_STM_DT)\n",
    "Y_test_STM_post_DT = np.expm1(Y_test_STM_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_STM, X_test_pca_STM = makePCA(X_train_STM, Y_train_STM, X_test_STM, featureNames_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on non-encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_STM_DT, X_test_pca_STM_DT = makePCA(X_train_STM_DT, Y_train_STM_DT, X_test_STM_DT, featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "lr_pca_RMSE_STM, lr_pca_MAE_STM = makeLinearRegression(X_train_pca_STM, Y_train_STM, Y_train_STM_post, X_test_pca_STM, Y_test_STM_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "lr_RMSE_STM, lr_MAE_STM = makeLinearRegression(X_train_STM, Y_train_STM, Y_train_STM_post, X_test_STM, Y_test_STM_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "dt_pca_RMSE_STM, dt_pca_MAE_STM = makeDecisionTree(X_train_pca_STM_DT, Y_train_STM_DT, Y_train_STM_post_DT, X_test_pca_STM_DT, Y_test_STM_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "dt_RMSE_STM, dt_MAE_STM = makeDecisionTree(X_train_STM_DT, Y_train_STM_DT, Y_train_STM_post_DT, X_test_STM_DT, Y_test_STM_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "rf_pca_RMSE_STM, rf_pca_MAE_STM = makeRandomForest(X_train_pca_STM_DT, Y_train_STM_DT, Y_train_STM_post_DT, X_test_pca_STM_DT, Y_test_STM_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "rf_RMSE_STM, rf_MAE_STM = makeRandomForest(X_train_STM_DT, Y_train_STM_DT, Y_train_STM_post_DT, X_test_STM_DT, Y_test_STM_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "svr_pca_RMSE_STM, svr_pca_MAE_STM = makeSVR(X_train_pca_STM, Y_train_STM, Y_train_STM_post, X_test_pca_STM, Y_test_STM_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "svr_RMSE_STM, svr_MAE_STM = makeSVR(X_train_STM, Y_train_STM, Y_train_STM_post, X_test_STM, Y_test_STM_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "knn_pca_RMSE_STM, knn_pca_MAE_STM = makeKNN(X_train_pca_STM, Y_train_STM, Y_train_STM_post, X_test_pca_STM, Y_test_STM_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "knn_RMSE_STM, knn_MAE_STM = makeKNN(X_train_STM, Y_train_STM, Y_train_STM_post, X_test_STM, Y_test_STM_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result with PCA\n",
    "RMSEs = [lr_pca_RMSE_STM, dt_pca_RMSE_STM, rf_pca_RMSE_STM, svr_pca_RMSE_STM, knn_pca_RMSE_STM] \n",
    "MAEs = [lr_pca_MAE_STM, dt_pca_MAE_STM, rf_pca_MAE_STM, svr_pca_MAE_STM, knn_pca_MAE_STM]\n",
    "plotResult(RMSEs, MAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = [lr_RMSE_STM, dt_RMSE_STM, rf_RMSE_STM, svr_RMSE_STM, knn_RMSE_STM]  \n",
    "MAEs = [lr_MAE_STM, dt_MAE_STM, rf_MAE_STM, svr_MAE_STM, knn_MAE_STM]\n",
    "RMSEs_pca = [lr_pca_RMSE_STM, dt_pca_RMSE_STM, rf_pca_RMSE_STM, svr_pca_RMSE_STM, knn_pca_RMSE_STM] \n",
    "MAEs_pca = [lr_pca_MAE_STM, dt_pca_MAE_STM, rf_pca_MAE_STM, svr_pca_MAE_STM, knn_pca_MAE_STM]\n",
    "plotResult(RMSEs, RMSEs_pca, MAEs, MAEs_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 FWI <a class=\"anchor\" id=\"5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FWI – using only the four FWI components:\n",
    "- FFMC\n",
    "- DMC\n",
    "- DC\n",
    "- ISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FWI = df_dummy[['FFMC', 'DMC', 'DC', 'ISI']]\n",
    "featureNames_dummy = list(X_FWI)\n",
    "X_FWI = X_FWI.values\n",
    "\n",
    "X_FWI_DT = df[['FFMC', 'DMC', 'DC', 'ISI']]\n",
    "featureNames = list(X_FWI_DT)\n",
    "X_FWI_DT = X_FWI_DT.values\n",
    "\n",
    "Y_FWI = df['area'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_FWI, X_test_FWI, Y_train_FWI, Y_test_FWI = train_test_split( X_FWI, Y_FWI, test_size=0.30, random_state=0)\n",
    "X_train_FWI_DT, X_test_FWI_DT, Y_train_FWI_DT, Y_test_FWI_DT = train_test_split( X_FWI_DT, Y_FWI, test_size=0.30, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train_FWI)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train_FWI = scaler.transform(X_train_FWI)\n",
    "X_test_FWI = scaler.transform(X_test_FWI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute post-processed Ys\n",
    "Y_train_FWI_post = np.expm1(Y_train_FWI)\n",
    "Y_test_FWI_post = np.expm1(Y_test_FWI)\n",
    "Y_train_FWI_post_DT = np.expm1(Y_train_FWI_DT)\n",
    "Y_test_FWI_post_DT = np.expm1(Y_test_FWI_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_FWI, X_test_pca_FWI = makePCA(X_train_FWI, Y_train_FWI, X_test_FWI, featureNames_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on non-encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_FWI_DT, X_test_pca_FWI_DT = makePCA(X_train_FWI_DT, Y_train_FWI_DT, X_test_FWI_DT, featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "lr_pca_RMSE_FWI, lr_pca_MAE_FWI = makeLinearRegression(X_train_pca_FWI, Y_train_FWI, Y_train_FWI_post, X_test_pca_FWI, Y_test_FWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "lr_RMSE_FWI, lr_MAE_FWI = makeLinearRegression(X_train_FWI, Y_train_FWI, Y_train_FWI_post, X_test_FWI, Y_test_FWI_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "dt_pca_RMSE_FWI, dt_pca_MAE_FWI = makeDecisionTree(X_train_pca_FWI_DT, Y_train_FWI_DT, Y_train_FWI_post_DT, X_test_pca_FWI_DT, Y_test_FWI_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "dt_RMSE_FWI, dt_MAE_FWI = makeDecisionTree(X_train_FWI_DT, Y_train_FWI_DT, Y_train_FWI_post_DT, X_test_FWI_DT, Y_test_FWI_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "rf_pca_RMSE_FWI, rf_pca_MAE_FWI = makeRandomForest(X_train_pca_FWI_DT, Y_train_FWI_DT, Y_train_FWI_post_DT, X_test_pca_FWI_DT, Y_test_FWI_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "rf_RMSE_FWI, rf_MAE_FWI = makeRandomForest(X_train_FWI_DT, Y_train_FWI_DT, Y_train_FWI_post_DT, X_test_FWI_DT, Y_test_FWI_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "svr_pca_RMSE_FWI, svr_pca_MAE_FWI = makeSVR(X_train_pca_FWI, Y_train_FWI, Y_train_FWI_post, X_test_pca_FWI, Y_test_FWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "svr_RMSE_FWI, svr_MAE_FWI = makeSVR(X_train_FWI, Y_train_FWI, Y_train_FWI_post, X_test_FWI, Y_test_FWI_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "knn_pca_RMSE_FWI, knn_pca_MAE_FWI = makeKNN(X_train_pca_FWI, Y_train_FWI, Y_train_FWI_post, X_test_pca_FWI, Y_test_FWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "knn_RMSE_FWI, knn_MAE_FWI = makeKNN(X_train_FWI, Y_train_FWI, Y_train_FWI_post, X_test_FWI, Y_test_FWI_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = [lr_RMSE_FWI, dt_RMSE_FWI, rf_RMSE_FWI, svr_RMSE_FWI, knn_RMSE_FWI]  \n",
    "MAEs = [lr_MAE_FWI, dt_MAE_FWI, rf_MAE_FWI, svr_MAE_FWI, knn_MAE_FWI]\n",
    "RMSEs_pca = [lr_pca_RMSE_FWI, dt_pca_RMSE_FWI, rf_pca_RMSE_FWI, svr_pca_RMSE_FWI, knn_pca_RMSE_FWI] \n",
    "MAEs_pca = [lr_pca_MAE_FWI, dt_pca_MAE_FWI, rf_pca_MAE_FWI, svr_pca_MAE_FWI, knn_pca_MAE_FWI]\n",
    "plotResult(RMSEs, RMSEs_pca, MAEs, MAEs_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Weather Conditions <a class=\"anchor\" id=\"5.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M – with the weather conditions:\n",
    "- temp\n",
    "- RH\n",
    "- wind\n",
    "- rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_M = df_dummy[['temp', 'RH', 'wind', 'rain']]\n",
    "featureNames_dummy = list(X_M)\n",
    "X_M = X_M.values\n",
    "\n",
    "X_M_DT = df[['temp', 'RH', 'wind', 'rain']]\n",
    "featureNames = list(X_M_DT)\n",
    "X_M_DT = X_M_DT.values\n",
    "\n",
    "Y_M = df['area'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_M, X_test_M, Y_train_M, Y_test_M = train_test_split( X_M, Y_M, test_size=0.30, random_state=0)\n",
    "X_train_M_DT, X_test_M_DT, Y_train_M_DT, Y_test_M_DT = train_test_split( X_M_DT, Y_M, test_size=0.30, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train_M)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train_M = scaler.transform(X_train_M)\n",
    "X_test_M = scaler.transform(X_test_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute post-processed Ys\n",
    "Y_train_M_post = np.expm1(Y_train_M)\n",
    "Y_test_M_post = np.expm1(Y_test_M)\n",
    "Y_train_M_post_DT = np.expm1(Y_train_M_DT)\n",
    "Y_test_M_post_DT = np.expm1(Y_test_M_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_M, X_test_pca_M = makePCA(X_train_M, Y_train_M, X_test_M, featureNames_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA result on non-encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_M_DT, X_test_pca_M_DT = makePCA(X_train_M_DT, Y_train_M_DT, X_test_M_DT, featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "lr_pca_RMSE_M, lr_pca_MAE_M = makeLinearRegression(X_train_pca_M, Y_train_M, Y_train_M_post, X_test_pca_M, Y_test_M_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "lr_RMSE_M, lr_MAE_M = makeLinearRegression(X_train_M, Y_train_M, Y_train_M_post, X_test_M, Y_test_M_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "dt_pca_RMSE_M, dt_pca_MAE_M = makeDecisionTree(X_train_pca_M_DT, Y_train_M_DT, Y_train_M_post_DT, X_test_pca_M_DT, Y_test_M_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "dt_RMSE_M, dt_MAE_M = makeDecisionTree(X_train_M_DT, Y_train_M_DT, Y_train_M_post_DT, X_test_M_DT, Y_test_M_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "rf_pca_RMSE_M, rf_pca_MAE_M = makeRandomForest(X_train_pca_M_DT, Y_train_M_DT, Y_train_M_post_DT, X_test_pca_M_DT, Y_test_M_post_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "rf_RMSE_M, rf_MAE_M = makeRandomForest(X_train_M_DT, Y_train_M_DT, Y_train_M_post_DT, X_test_M_DT, Y_test_M_post_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "svr_pca_RMSE_M, svr_pca_MAE_M = makeSVR(X_train_pca_M, Y_train_M, Y_train_M_post, X_test_pca_M, Y_test_M_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "svr_RMSE_M, svr_MAE_M = makeSVR(X_train_M, Y_train_M, Y_train_M_post, X_test_M, Y_test_M_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With PCA:\")\n",
    "knn_pca_RMSE_M, knn_pca_MAE_M = makeKNN(X_train_pca_M, Y_train_M, Y_train_M_post, X_test_pca_M, Y_test_M_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA:\")\n",
    "knn_RMSE_M, knn_MAE_M =makeKNN(X_train_M, Y_train_M, Y_train_M_post, X_test_M, Y_test_M_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = [lr_RMSE_M, dt_RMSE_M, rf_RMSE_M, svr_RMSE_M, knn_RMSE_M]  \n",
    "MAEs = [lr_MAE_FWI, dt_MAE_M, rf_MAE_M, svr_MAE_M, knn_MAE_M]\n",
    "RMSEs_pca = [lr_pca_RMSE_M, dt_pca_RMSE_M, rf_pca_RMSE_M, svr_pca_RMSE_M, knn_pca_RMSE_M] \n",
    "MAEs_pca = [lr_pca_MAE_M, dt_pca_MAE_M, rf_pca_MAE_M, svr_pca_MAE_M, knn_pca_MAE_M]\n",
    "plotResult(RMSEs, RMSEs_pca, MAEs, MAEs_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions <a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of obtained resuslts divided by metrics and PCA/non-PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'Algorithms' ,'Normal', 'STFWI', 'STM', 'FWI', 'M']\n",
    "values = [['Linear Regression', '{:0.4f}'.format(lr_pca_RMSE), '{:0.4f}'.format(lr_pca_RMSE_STFWI), '{:0.4f}'.format(lr_pca_RMSE_STM), '{:0.4f}'.format(lr_pca_RMSE_FWI), '{:0.4f}'.format(lr_pca_RMSE_M)],\n",
    "         ['Regression Tree', '{:0.4f}'.format(dt_pca_RMSE), '{:0.4f}'.format(dt_pca_RMSE_STFWI), '{:0.4f}'.format(dt_pca_RMSE_STM), '{:0.4f}'.format(dt_pca_RMSE_FWI), '{:0.4f}'.format(dt_pca_RMSE_M)],\n",
    "         ['Random forest', '{:0.4f}'.format(rf_pca_RMSE), '{:0.4f}'.format(rf_pca_RMSE_STFWI), '{:0.4f}'.format(rf_pca_RMSE_STM), '{:0.4f}'.format(rf_pca_RMSE_FWI), '{:0.4f}'.format(rf_pca_RMSE_M)],\n",
    "         ['SVR', '{:0.4f}'.format(svr_pca_RMSE), '{:0.4f}'.format(svr_pca_RMSE_STFWI), '{:0.4f}'.format(svr_pca_RMSE_STM), '{:0.4f}'.format(svr_pca_RMSE_FWI), '{:0.4f}'.format(svr_pca_RMSE_M)],\n",
    "         ['K-NN', '{:0.4f}'.format(knn_pca_RMSE), '{:0.4f}'.format(knn_pca_RMSE_STFWI), '{:0.4f}'.format(knn_pca_RMSE_STM), '{:0.4f}'.format(knn_pca_RMSE_FWI), '{:0.4f}'.format(knn_pca_RMSE_M)]]\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "#fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "ax.table(cellText=values, colLabels=columns, loc='center')\n",
    "fig.tight_layout()\n",
    "plt.title(\"RMSE with PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'Algorithms' ,'Normal', 'STFWI', 'STM', 'FWI', 'M']\n",
    "values = [['Linear Regression', '{:0.4f}'.format(lr_RMSE), '{:0.4f}'.format(lr_RMSE_STFWI), '{:0.4f}'.format(lr_RMSE_STM), '{:0.4f}'.format(lr_RMSE_FWI), '{:0.4f}'.format(lr_RMSE_M)],\n",
    "         ['Regression Tree', '{:0.4f}'.format(dt_RMSE), '{:0.4f}'.format(dt_RMSE_STFWI), '{:0.4f}'.format(dt_RMSE_STM), '{:0.4f}'.format(dt_RMSE_FWI), '{:0.4f}'.format(dt_RMSE_M)],\n",
    "         ['Random forest', '{:0.4f}'.format(rf_RMSE), '{:0.4f}'.format(rf_RMSE_STFWI), '{:0.4f}'.format(rf_RMSE_STM), '{:0.4f}'.format(rf_RMSE_FWI), '{:0.4f}'.format(rf_RMSE_M)],\n",
    "         ['SVR', '{:0.4f}'.format(svr_RMSE), '{:0.4f}'.format(svr_RMSE_STFWI), '{:0.4f}'.format(svr_RMSE_STM), '{:0.4f}'.format(svr_RMSE_FWI), '{:0.4f}'.format(svr_RMSE_M)],\n",
    "         ['K-NN', '{:0.4f}'.format(knn_RMSE), '{:0.4f}'.format(knn_RMSE_STFWI), '{:0.4f}'.format(knn_RMSE_STM), '{:0.4f}'.format(knn_RMSE_FWI), '{:0.4f}'.format(knn_RMSE_M)]]\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "#fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "ax.table(cellText=values, colLabels=columns, loc='center')\n",
    "fig.tight_layout()\n",
    "plt.title(\"RMSE without PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'Algorithms' ,'Normal', 'STFWI', 'STM', 'FWI', 'M']\n",
    "values = [['Linear Regression', '{:0.4f}'.format(lr_pca_MAE), '{:0.4f}'.format(lr_pca_MAE_STFWI), '{:0.4f}'.format(lr_pca_MAE_STM), '{:0.4f}'.format(lr_pca_MAE_FWI), '{:0.4f}'.format(lr_pca_MAE_M)],\n",
    "         ['Regression Tree', '{:0.4f}'.format(dt_pca_MAE), '{:0.4f}'.format(dt_pca_MAE_STFWI), '{:0.4f}'.format(dt_pca_MAE_STM), '{:0.4f}'.format(dt_pca_MAE_FWI), '{:0.4f}'.format(dt_pca_MAE_M)],\n",
    "         ['Random forest', '{:0.4f}'.format(rf_pca_MAE), '{:0.4f}'.format(rf_pca_MAE_STFWI), '{:0.4f}'.format(rf_pca_MAE_STM), '{:0.4f}'.format(rf_pca_MAE_FWI), '{:0.4f}'.format(rf_pca_MAE_M)],\n",
    "         ['SVR', '{:0.4f}'.format(svr_pca_MAE), '{:0.4f}'.format(svr_pca_MAE_STFWI), '{:0.4f}'.format(svr_pca_MAE_STM), '{:0.4f}'.format(svr_pca_MAE_FWI), '{:0.4f}'.format(svr_pca_MAE_M)],\n",
    "         ['K-NN', '{:0.4f}'.format(knn_pca_MAE), '{:0.4f}'.format(knn_pca_MAE_STFWI), '{:0.4f}'.format(knn_pca_MAE_STM), '{:0.4f}'.format(knn_pca_MAE_FWI), '{:0.4f}'.format(knn_pca_MAE_M)]]\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "#fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "ax.table(cellText=values, colLabels=columns, loc='center')\n",
    "fig.tight_layout()\n",
    "plt.title(\"MAE with PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'Algorithms' ,'Normal', 'STFWI', 'STM', 'FWI', 'M']\n",
    "values = [['Linear Regression', '{:0.4f}'.format(lr_MAE), '{:0.4f}'.format(lr_MAE_STFWI), '{:0.4f}'.format(lr_MAE_STM), '{:0.4f}'.format(lr_MAE_FWI), '{:0.4f}'.format(lr_MAE_M)],\n",
    "         ['Regression Tree', '{:0.4f}'.format(dt_MAE), '{:0.4f}'.format(dt_MAE_STFWI), '{:0.4f}'.format(dt_MAE_STM), '{:0.4f}'.format(dt_MAE_FWI), '{:0.4f}'.format(dt_MAE_M)],\n",
    "         ['Random forest', '{:0.4f}'.format(rf_MAE), '{:0.4f}'.format(rf_MAE_STFWI), '{:0.4f}'.format(rf_MAE_STM), '{:0.4f}'.format(rf_MAE_FWI), '{:0.4f}'.format(rf_MAE_M)],\n",
    "         ['SVR', '{:0.4f}'.format(svr_MAE), '{:0.4f}'.format(svr_MAE_STFWI), '{:0.4f}'.format(svr_MAE_STM), '{:0.4f}'.format(svr_MAE_FWI), '{:0.4f}'.format(svr_MAE_M)],\n",
    "         ['K-NN', '{:0.4f}'.format(knn_MAE), '{:0.4f}'.format(knn_MAE_STFWI), '{:0.4f}'.format(knn_MAE_STM), '{:0.4f}'.format(knn_MAE_FWI), '{:0.4f}'.format(knn_MAE_M)]]\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "#fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "ax.table(cellText=values, colLabels=columns, loc='center')\n",
    "fig.tight_layout()\n",
    "plt.title(\"MAE without PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, our models performed in similar ways, relatively to each features selection. The normal dataset (i.e.: the one with all features) generalizes better then other ones, given the fact that this dataset was generated ad hoc for this kind of prediction, starting from different sources.\n",
    "\n",
    "Our analysis was based on two different types of metrics (i.e.: MAE, RMSE). Results are quite different, depending on considered metric. One of the reasons is that RMSE is more sensitive to high errors.\n",
    "\n",
    "The purpose of referenced paper was to demonstrate that burnt area can be predicted only from metereological data. We confirmed their thesis, obtaining good results with all algorithms and \"M\" feature selection. In this case, the best performing algorithm was Random Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
